## Wikipedia Data Crawling and Processing

# Project Overview
This Python-based project is designed to crawl data from Wikipedia using Apache Airflow, clean the data, and push it to Azure Data Lake and Azure Data Factory for further processing. The goal of this project is to build an end-to-end data pipeline that efficiently ingests, cleans, and stores data in a cloud environment, leveraging the power of Azure for storage and data processing

# System Architecture
![System Architecture](/system_architecture.png)

# Tableau Dashboard
![Tableau Dashboard](/Screenshot 2025-01-15 at 11.57.31â€¯PM.png)


# Key Technologies Used
- Apache Airflow: Used for orchestrating and automating the workflow.
- Python: The primary language for implementing the crawling and data processing logic.
- Wikipedia API: For crawling data from Wikipedia pages.
- Azure Data Lake: For storing the crawled and processed data.
- Azure Data Factory: For data transformation and further integration with other data processing tools.

  
